from pydantic import BaseModel # For creating structured data models (JSON) to pass to GPT
from openai import OpenAI 
from promptHelper import getTweets, getUsernames
import re
from dotenv import load_dotenv
import os
import json

# Get environment variable containing the OpenAI API key, which is manually passed through the Swagger API
load_dotenv()
OPENAI_API_KEY = os.getenv('ENV_VAR1')
client = OpenAI(api_key=OPENAI_API_KEY)

# Choose the OpenAI model to use
MODEL_NAME = "gpt-4o-mini"
ENCODING_NAME = "o200k_base"

# Define number of tweets and users to generate
TWEETS_SIZE=60
USERS_SIZE=20

# Define the files to save the generated tweets and users
tweetSaveFile="tweets_response.json"
userSaveFile="users_response.txt"

# Used to interact with OpenAI's chat completion API. Passes in a chat history, specifies API model and parameters, then returns API response as text
def getResponse(theMessages, model_name=MODEL_NAME, out_response_format="text"):
    response = client.chat.completions.create(
        model=model_name,
        messages=theMessages, # The tweet messages previously generated by the OpenAI model. This will act like the chat history.
        response_format={
            "type": out_response_format 
        },
        temperature=1, # Average temperature, for a balance of between predictable and random output
        max_completion_tokens=16384, # Max number of tokens (i.e. a word, part of a word, or punctuation) to be generated in response
        presence_penalty=0.0 # Controls repetition. 0.0 is no penalty; the model freely repeats words or topics if they seem relevant.
    )
    return response


def save_results_to_file(generatedResults, file):
    # Overwrite previous file
    with open(file, "w", encoding="utf-8") as f:
        f.write(generatedResults)
    print(f"Content saved to {file}")

# Generate the tweets which will be passed as chat history into OpenAI's chat completion API
def generateTweets(tweet_sample_size=TWEETS_SIZE):

    # Get a random subset of real user tweets from the competition's subsession info
    tweets_dataset=getTweets(tweet_sample_size)

    # First part of the GPT prompt - describes the "persona" it should take on
    system_prompt = "You are a creative assistant that generates realistic tweets."

    # Second part of the GPT prompt - the task it should respond to
    user_prompt = f"""
                Generate tweets in a similar format to the following dataset.:
                {tweets_dataset}
                """
    # user_prompt = f"""
    #             Generate 50 tweets in a similar format to the following dataset. Return in a json format:
    #             {tweets_dataset}
    #             """
    
    # Combines the system and user prompts into a single message to send to the OpenAI API
    tweet_message=messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    # How do we know how many tweets will be generated? No
    tweets_response = getResponse(tweet_message)
 
    # Extract the tweets json, and write it to a file
    tweets_response_content=tweets_response.choices[0].message.content
    save_results_to_file(tweets_response_content,tweetSaveFile)

    # Probably delete this
    tweets_data = json.loads(tweets_response_content)

    return tweets_data['tweets']


def generateUsers(user_sample_size=USERS_SIZE):
    users_dataset=getUsernames(user_sample_size)

    # Prompt to generate new users
    user_prompt = f"""
                Generate user metadata in a similar format to the following dataset.:
                {users_dataset}
                Format it as json, but remove the triple backticks and language specifier.
                """

    user_messages=[
        {"role": "system", "content": "You are a creative assistant that generates realistic twitter user metadata."},
        {"role": "user", "content": user_prompt}
    ]

    users_response = getResponse(user_messages, out_response_format="text")
    users_response_content=users_response.choices[0].message.content
    data = json.loads(users_response_content)

    # Saves to users_response.json
    save_results_to_file(users_response_content,userSaveFile)

    parsed_users = []
    for each_user in data["users"]:
        username = each_user["username"]
        name = each_user["name"]
        description = each_user["description"]
        location = each_user["location"]

        if name and username and description and location:
            parsed_users.append({"name": name, "username": username, "description": description, "location": location})
    return parsed_users

# print(generateUsers())

# Print the parsed data
# for user in parsed_users:
#     print(f"Name: {user['name']}")
#     print(f"Username: {user['username']}")
#     print(f"Description: {user['description']}")
#     print("-" * 40)